# Generated by Django 5.2 on 2025-09-05 12:34

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('api', '0111_add_token_usage_to_persistent_agent_step'),
    ]

    operations = [
        migrations.AddField(
            model_name='browseruseagenttask',
            name='cached_tokens',
            field=models.IntegerField(blank=True, help_text='Number of cached tokens used (if provider supports caching)', null=True),
        ),
        migrations.AddField(
            model_name='browseruseagenttask',
            name='completion_tokens',
            field=models.IntegerField(blank=True, help_text="Number of tokens generated in the completion for this step's LLM call", null=True),
        ),
        migrations.AddField(
            model_name='browseruseagenttask',
            name='llm_model',
            field=models.CharField(blank=True, help_text="LLM model used for this step (e.g., 'claude-3-opus-20240229')", max_length=256, null=True),
        ),
        migrations.AddField(
            model_name='browseruseagenttask',
            name='llm_provider',
            field=models.CharField(blank=True, help_text="LLM provider used for this step (e.g., 'anthropic', 'openai')", max_length=128, null=True),
        ),
        migrations.AddField(
            model_name='browseruseagenttask',
            name='prompt_tokens',
            field=models.IntegerField(blank=True, help_text="Number of tokens used in the prompt for this step's LLM call", null=True),
        ),
        migrations.AddField(
            model_name='browseruseagenttask',
            name='total_tokens',
            field=models.IntegerField(blank=True, help_text="Total tokens used (prompt + completion) for this step's LLM call", null=True),
        ),
    ]
